{
 "metadata": {
  "name": "",
  "signature": "sha256:5f088aa09ece4c868c0227f55893d711723c04870addf87a331c9773fc5a1202"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data Science School, Nyeri, Kenya\n",
      "\n",
      "### 15th-17th June 2015\n",
      "\n",
      "### Neil D. Lawrence\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Welcome to the data science school in Nyeri, Kenya.\n",
      "\n",
      "This notebook provides you with the guide to your lab classes for the entire school. The lab classes are intended to help get you familiar with modeling as well as the principles of probabilistic inference.\n",
      "\n",
      "The lab classes make use of our `pods` software for ['open data science'](http://inverseprobability.com/2014/07/01/open-data-science/) for access to data sets and other resources.\n",
      "\n",
      "```sh\n",
      "pip install -pre pods\n",
      "```\n",
      "\n",
      "## Background and Probability Review\n",
      "\n",
      "The first day will review probability and introduce matrix factorisation as example of objective function minimization, moving on to linear regression.\n",
      "\n",
      "* [Jupyter and Probability Review](./jupyter and probability intro.ipynb) Introduction to the Jupyter notebook and a review of probability.\n",
      "* [Matrix Factorization for Collaborative Filtering](./matrix factorization.ipynb) An example of using `pandas` for data analysis and how to represent opinions on a computer: collaborative filtering and matrix factorization.\n",
      "* [Introduction to Regression and Linear Algebra Review](./regression.ipynb) Regression is the mainstay of many approaches to machine learning, here we also motivate linear algebra through solving the regression problem.\n",
      "\n",
      "\n",
      "## Basis Functions and Probabilistic Regression\n",
      "\n",
      "The second day will focus on probabilistic interpretations of regression and model validation.\n",
      "\n",
      "* [Introduction to Basis Functions](./basis functions.ipynb) Linear models can be limiting, basis functions allow us to go non linear in our predictions, but stay linear in our parameters.\n",
      "* [Model Validation](./model validation.ipynb) Validation of model predictions is one of the most general and important concepts in machine learning, statistics and data science. Here we teach the general concepts and apply them to polynomial regression.\n",
      "* [Bayesian Regression](./Bayesian regression.ipynb) Bayesian averaging over models is one way of improving performance through reducing variance, but without increasing bias.\n",
      "\n",
      "## Dimensionality Reduction and Classification\n",
      "\n",
      "The third day focusses on dimensionality reduction and classification.\n",
      "\n",
      "* [Dimensionality reduction](./dimensionality reduction.ipynb) Unsupervised learning is an exploratory approach to understanding a data set. Here we consider dimensionality reduction as an approach to unsupervised learning.\n",
      "* [Probabilistic classification and naive Bayes](./probabilistic classification.ipynb) Classification of data is a mainstay of machine learning and data science. Here we consider the naive Bayes approach from the perspective of probabilistic modeling. \n",
      "* [Logistic regression and Generalized Linear Models](./logistic regression.ipynb) Naive Bayes classifies the data by modeling the entire joint distribution of the the labels and inputs, this can be useful when there's missing data, but it requires a very rich class of models. Logistic regression models the conditional distribution of the label given the data. It also leads to a very general class of models know as 'generalized linear models'.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}